{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ba4762e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ferpaolo/Programming/Thesis/tf-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-10-18 15:27:37.230030: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-18 15:27:38.043594: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-18 15:27:41.060368: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, log_loss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e88e2cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensured directory 'models/' exists.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "RANDOM_SEED = 42\n",
    "MODEL_DIR = 'models/'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True) # Ensure directory exists\n",
    "EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2' \n",
    "print(f\"Ensured directory '{MODEL_DIR}' exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd89d6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SBERT Model...\n",
      "SBERT Model Loaded. Embedding Dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# --- Initialize Models (Run once) ---\n",
    "print(\"Loading SBERT Model...\")\n",
    "SBERT_MODEL = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "EMBEDDING_DIM = SBERT_MODEL.get_sentence_embedding_dimension()\n",
    "print(f\"SBERT Model Loaded. Embedding Dimension: {EMBEDDING_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc47af78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hybrid Feature Extraction ---\n",
    "\n",
    "def get_semantic_embedding(text):\n",
    "    \"\"\"Generates the semantic embedding vector using SBERT.\"\"\"\n",
    "    # SBERT can process lists, which is more efficient\n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    return SBERT_MODEL.encode(text, convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae6b6cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_hybrid_features(df, fitted_tfidf, sbert_model):\n",
    "    \"\"\"\n",
    "    Transforms the JD-Resume pairs into the hybrid numerical feature matrix (X).\n",
    "    X_features will have columns: [TFIDF_Similarity, SBERT_Similarity]\n",
    "    \"\"\"\n",
    "    X_features = []\n",
    "\n",
    "    # Get embeddings for all JD and Resume texts in one batch (much faster)\n",
    "    all_jd_emb = sbert_model.encode(df['job_description'].tolist(), convert_to_numpy=True)\n",
    "    all_res_emb = sbert_model.encode(df['resume_text'].tolist(), convert_to_numpy=True)\n",
    "    \n",
    "    # Get TF-IDF vectors for all texts in one batch\n",
    "    all_jd_tfidf = fitted_tfidf.transform(df['job_description'].tolist())\n",
    "    all_res_tfidf = fitted_tfidf.transform(df['resume_text'].tolist())\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        # 1. Lexical Feature (TF-IDF Similarity)\n",
    "        # Cosine similarity between JD and Resume TF-IDF vectors\n",
    "        tfidf_similarity = cosine_similarity(all_jd_tfidf[i], all_res_tfidf[i])[0][0]\n",
    "        \n",
    "        # 2. Semantic Feature (SBERT Similarity)\n",
    "        # Cosine similarity between JD and Resume SBERT embeddings\n",
    "        # Reshape is needed for cosine_similarity function if only one vector\n",
    "        semantic_similarity = cosine_similarity(\n",
    "            all_jd_emb[i].reshape(1, -1), \n",
    "            all_res_emb[i].reshape(1, -1)\n",
    "        )[0][0]\n",
    "\n",
    "        # 3. Concatenate the final feature vector for XGBoost\n",
    "        feature_vector = [tfidf_similarity, semantic_similarity]\n",
    "        X_features.append(feature_vector)\n",
    "        \n",
    "    return np.array(X_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89a8c285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size: 590 samples\n",
      "Fit (1) examples: 295\n",
      "No Fit (0) examples: 295\n"
     ]
    }
   ],
   "source": [
    "# Load the new data\n",
    "df = pd.read_csv(\"job_resume_pairs.csv\")\n",
    "\n",
    "# --- Use the explicit columns ---\n",
    "# X_text now contains the two text columns\n",
    "X_text = df[['job_description', 'resume_text']]\n",
    "# y is the explicit label\n",
    "y = df['label']\n",
    "\n",
    "print(f\"Dataset Size: {len(df)} samples\")\n",
    "print(f\"Fit (1) examples: {df['label'].sum()}\")\n",
    "print(f\"No Fit (0) examples: {len(df) - df['label'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4963cfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Training Set Size: 354\n",
      "Validation Set Size: 118\n",
      "Test Set Size: 118\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 1. Split into Training Pool (80%) and Final Test Set (20%)\n",
    "X_pool, X_test, y_pool, y_test = train_test_split(\n",
    "    X_text, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n",
    ")\n",
    "\n",
    "# 2. Split Training Pool into Training Set (75% of pool) and Validation Set (25% of pool)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_pool, y_pool, test_size=(0.25), random_state=RANDOM_SEED, stratify=y_pool\n",
    ")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Training Set Size: {len(X_train)}\")\n",
    "print(f\"Validation Set Size: {len(X_val)}\")\n",
    "print(f\"Test Set Size: {len(X_test)}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86c93d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TF-IDF Vectorizer on ALL Training Text...\n",
      "TF-IDF Model saved to models/...\n",
      "Extracting Hybrid Features for all three sets...\n",
      "Training XGBoost Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ferpaolo/Programming/Thesis/tf-env/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [15:28:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Model saved to models/...\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Feature Transformation and XGBoost Training ---\n",
    "\n",
    "# --- STEP 1: TRAIN AND SAVE TF-IDF VECTORIZER (Lexical Model) ---\n",
    "print(\"Fitting TF-IDF Vectorizer on ALL Training Text...\")\n",
    "# Concatenate JD and Resume text to form the corpus for TF-IDF fitting\n",
    "train_corpus = X_train['job_description'].tolist() + X_train['resume_text'].tolist()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "# FIT ONLY ON TRAINING DATA's corpus!\n",
    "tfidf_vectorizer.fit(train_corpus) \n",
    "\n",
    "# Save the fitted TF-IDF model\n",
    "joblib.dump(tfidf_vectorizer, MODEL_DIR + 'tfidf_vectorizer.pkl') \n",
    "print(f\"TF-IDF Model saved to {MODEL_DIR}...\")\n",
    "\n",
    "\n",
    "# --- STEP 2: HYBRID FEATURE EXTRACTION ---\n",
    "print(\"Extracting Hybrid Features for all three sets...\")\n",
    "# Transform all three sets using the *fitted* TF-IDF and SBERT\n",
    "X_train_hybrid = combine_hybrid_features(X_train, tfidf_vectorizer, SBERT_MODEL)\n",
    "X_val_hybrid = combine_hybrid_features(X_val, tfidf_vectorizer, SBERT_MODEL)\n",
    "X_test_hybrid = combine_hybrid_features(X_test, tfidf_vectorizer, SBERT_MODEL)\n",
    "\n",
    "\n",
    "# --- STEP 3: TRAIN XGBOOST CLASSIFIER (Balanced Data) ---\n",
    "print(\"Training XGBoost Classifier...\")\n",
    "xgb_model = XGBClassifier(\n",
    "    use_label_encoder=False, \n",
    "    eval_metric='logloss', \n",
    "    random_state=RANDOM_SEED,\n",
    "    # Set to a fixed high number since we are NOT using early_stopping_rounds in v3.0.5\n",
    "    n_estimators=1000, \n",
    "    # scale_pos_weight is omitted because the data is balanced (1:1)\n",
    ")\n",
    "\n",
    "# Train the model with the balanced, transformed training data\n",
    "xgb_model.fit(\n",
    "    X_train_hybrid, y_train,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Save the trained XGBoost model\n",
    "joblib.dump(xgb_model, MODEL_DIR + 'xgb_classifier.pkl')\n",
    "print(f\"XGBoost Model saved to {MODEL_DIR}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df23a489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "  FINAL MODEL EVALUATION (TEST SET)\n",
      "========================================\n",
      "Accuracy on Test Set: 0.9576\n",
      "F1-Score on Test Set: 0.9573\n",
      "Log Loss on Test Set: 0.1959\n",
      "\n",
      "Classification Report (ISO-IEC 25010 Metrics):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Fit (0)       0.95      0.97      0.96        59\n",
      "     Fit (1)       0.97      0.95      0.96        59\n",
      "\n",
      "    accuracy                           0.96       118\n",
      "   macro avg       0.96      0.96      0.96       118\n",
      "weighted avg       0.96      0.96      0.96       118\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"  FINAL MODEL EVALUATION (TEST SET)\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Predict on the unseen Test Set\n",
    "y_pred = xgb_model.predict(X_test_hybrid)\n",
    "y_proba = xgb_model.predict_proba(X_test_hybrid)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "logloss = log_loss(y_test, y_proba)\n",
    "\n",
    "\n",
    "print(f\"Accuracy on Test Set: {accuracy:.4f}\")\n",
    "print(f\"F1-Score on Test Set: {f1:.4f}\")\n",
    "print(f\"Log Loss on Test Set: {logloss:.4f}\")\n",
    "\n",
    "# Detailed report for thesis\n",
    "print(\"\\nClassification Report (ISO-IEC 25010 Metrics):\")\n",
    "print(classification_report(y_test, y_pred, target_names=['No Fit (0)', 'Fit (1)']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Thesis Project)",
   "language": "python",
   "name": "thesis-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
