{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba4762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, log_loss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88e2cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "RANDOM_SEED = 42\n",
    "MODEL_DIR = 'models/'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True) # Ensure directory exists\n",
    "EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2' \n",
    "print(f\"Ensured directory '{MODEL_DIR}' exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd89d6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize Models (Run once) ---\n",
    "print(\"Loading SBERT Model...\")\n",
    "SBERT_MODEL = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "EMBEDDING_DIM = SBERT_MODEL.get_sentence_embedding_dimension()\n",
    "print(f\"SBERT Model Loaded. Embedding Dimension: {EMBEDDING_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc47af78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hybrid Feature Extraction ---\n",
    "\n",
    "def get_semantic_embedding(text):\n",
    "    \"\"\"Generates the semantic embedding vector using SBERT.\"\"\"\n",
    "    # SBERT can process lists, which is more efficient\n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    return SBERT_MODEL.encode(text, convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b6cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_hybrid_features(df, fitted_tfidf, sbert_model):\n",
    "    \"\"\"\n",
    "    Transforms the JD-Resume pairs into the hybrid numerical feature matrix (X).\n",
    "    X_features will have columns: [TFIDF_Similarity, SBERT_Similarity]\n",
    "    \"\"\"\n",
    "    X_features = []\n",
    "\n",
    "    # Get embeddings for all JD and Resume texts in one batch (much faster)\n",
    "    all_jd_emb = sbert_model.encode(df['job_description'].tolist(), convert_to_numpy=True)\n",
    "    all_res_emb = sbert_model.encode(df['resume_text'].tolist(), convert_to_numpy=True)\n",
    "    \n",
    "    # Get TF-IDF vectors for all texts in one batch\n",
    "    all_jd_tfidf = fitted_tfidf.transform(df['job_description'].tolist())\n",
    "    all_res_tfidf = fitted_tfidf.transform(df['resume_text'].tolist())\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        # 1. Lexical Feature (TF-IDF Similarity)\n",
    "        # Cosine similarity between JD and Resume TF-IDF vectors\n",
    "        tfidf_similarity = cosine_similarity(all_jd_tfidf[i], all_res_tfidf[i])[0][0]\n",
    "        \n",
    "        # 2. Semantic Feature (SBERT Similarity)\n",
    "        # Cosine similarity between JD and Resume SBERT embeddings\n",
    "        # Reshape is needed for cosine_similarity function if only one vector\n",
    "        semantic_similarity = cosine_similarity(\n",
    "            all_jd_emb[i].reshape(1, -1), \n",
    "            all_res_emb[i].reshape(1, -1)\n",
    "        )[0][0]\n",
    "\n",
    "        # 3. Concatenate the final feature vector for XGBoost\n",
    "        feature_vector = [tfidf_similarity, semantic_similarity]\n",
    "        X_features.append(feature_vector)\n",
    "        \n",
    "    return np.array(X_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a8c285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new data\n",
    "df = pd.read_csv(\"job_resume_pairs.csv\")\n",
    "\n",
    "# --- Use the explicit columns ---\n",
    "# X_text now contains the two text columns\n",
    "X_text = df[['job_description', 'resume_text']]\n",
    "# y is the explicit label\n",
    "y = df['label']\n",
    "\n",
    "print(f\"Dataset Size: {len(df)} samples\")\n",
    "print(f\"Fit (1) examples: {df['label'].sum()}\")\n",
    "print(f\"No Fit (0) examples: {len(df) - df['label'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4963cfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Split into Training Pool (80%) and Final Test Set (20%)\n",
    "X_pool, X_test, y_pool, y_test = train_test_split(\n",
    "    X_text, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n",
    ")\n",
    "\n",
    "# 2. Split Training Pool into Training Set (75% of pool) and Validation Set (25% of pool)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_pool, y_pool, test_size=(0.25), random_state=RANDOM_SEED, stratify=y_pool\n",
    ")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Training Set Size: {len(X_train)}\")\n",
    "print(f\"Validation Set Size: {len(X_val)}\")\n",
    "print(f\"Test Set Size: {len(X_test)}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c93d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Feature Transformation and XGBoost Training ---\n",
    "\n",
    "# --- STEP 1: TRAIN AND SAVE TF-IDF VECTORIZER (Lexical Model) ---\n",
    "print(\"Fitting TF-IDF Vectorizer on ALL Training Text...\")\n",
    "# Concatenate JD and Resume text to form the corpus for TF-IDF fitting\n",
    "train_corpus = X_train['job_description'].tolist() + X_train['resume_text'].tolist()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "# FIT ONLY ON TRAINING DATA's corpus!\n",
    "tfidf_vectorizer.fit(train_corpus) \n",
    "\n",
    "# Save the fitted TF-IDF model\n",
    "joblib.dump(tfidf_vectorizer, MODEL_DIR + 'tfidf_vectorizer.pkl') \n",
    "print(f\"TF-IDF Model saved to {MODEL_DIR}...\")\n",
    "\n",
    "\n",
    "# --- STEP 2: HYBRID FEATURE EXTRACTION ---\n",
    "print(\"Extracting Hybrid Features for all three sets...\")\n",
    "# Transform all three sets using the *fitted* TF-IDF and SBERT\n",
    "X_train_hybrid = combine_hybrid_features(X_train, tfidf_vectorizer, SBERT_MODEL)\n",
    "X_val_hybrid = combine_hybrid_features(X_val, tfidf_vectorizer, SBERT_MODEL)\n",
    "X_test_hybrid = combine_hybrid_features(X_test, tfidf_vectorizer, SBERT_MODEL)\n",
    "\n",
    "\n",
    "# --- STEP 3: TRAIN XGBOOST CLASSIFIER (Balanced Data) ---\n",
    "print(\"Training XGBoost Classifier...\")\n",
    "xgb_model = XGBClassifier(\n",
    "    use_label_encoder=False, \n",
    "    eval_metric='logloss', \n",
    "    random_state=RANDOM_SEED,\n",
    "    # Set to a fixed high number since we are NOT using early_stopping_rounds in v3.0.5\n",
    "    n_estimators=1000, \n",
    "    # scale_pos_weight is omitted because the data is balanced (1:1)\n",
    ")\n",
    "\n",
    "# Train the model with the balanced, transformed training data\n",
    "xgb_model.fit(\n",
    "    X_train_hybrid, y_train,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Save the trained XGBoost model\n",
    "joblib.dump(xgb_model, MODEL_DIR + 'xgb_classifier.pkl')\n",
    "print(f\"XGBoost Model saved to {MODEL_DIR}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df23a489",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"  FINAL MODEL EVALUATION (TEST SET)\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Predict on the unseen Test Set\n",
    "y_pred = xgb_model.predict(X_test_hybrid)\n",
    "y_proba = xgb_model.predict_proba(X_test_hybrid)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "logloss = log_loss(y_test, y_proba)\n",
    "\n",
    "\n",
    "print(f\"Accuracy on Test Set: {accuracy:.4f}\")\n",
    "print(f\"F1-Score on Test Set: {f1:.4f}\")\n",
    "print(f\"Log Loss on Test Set: {logloss:.4f}\")\n",
    "\n",
    "# Detailed report for thesis\n",
    "print(\"\\nClassification Report (ISO-IEC 25010 Metrics):\")\n",
    "print(classification_report(y_test, y_pred, target_names=['No Fit (0)', 'Fit (1)']))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
